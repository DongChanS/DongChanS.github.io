---
layout : post
title : Automatic Speech Recognition (1) - Background and Fundamental theory
author : Shin Dong Chan
category : 'Speech Recognition'
---

이 포스트는 edX 의 [Speech Recognition Systems](https://courses.edx.org/courses/course-v1:Microsoft+DEV287x+2T2019/course/) 강의를 참고하여 만들어졌습니다.

## 1) Introduction

ASR(Automatic speech recognition) 시스템을 이해하기 위해서는 그와 관련된 여러 학문 분야(inter-disciplinary)를 알고 있어야 합니다.

이번에는 그와 관련된 기초 지식에 대해서 학습할 것입니다.

- 사람이 소리를 내기까지의 과정
- 여러 종류의 소리들
- Speech의 구조
  - Word, Syllable, phoneme
- 음성인식의 중요한 3가지 파트
  - Acoustic model, Language model, Decoding
- 음성인식 결과 측정 방법



## 32) Phonetics (음성학)

언어학(lingustics)의 일부, 사람의 음성을 연구하는 학문

### 2-1. 세부 분야

1. Articulatory(조음 기관) phonetics : **소리의 생성**을 연구함.
   - 성도(vocal tract, 성대 <-> 입술, 콧구멍)를 통과하여 소리가 생성됨.
2. Acoustic phonetics : **소리의 전달**(transmission)을 연구함.
3. Auditory(청각의) phonetics : **소리의 받아들임**(reception and perception)을 연구함.

### 2-2. phoneme (음소)

speech sound의 **가장 기본 단위**

- 한 단어는 **여러 sequence의 phoneme**들로 이루어져있음.

- phone(음, 음성) : acoustic realization of a phoneme

  - 이러한 realization은 **주변 음성(coarticulation)에 따라서 달라진다**
  - Allophones (이음, 의미차이는 나지 않지만 약간 다르게 들리는 동일 음소의 음) :  Coarticulation에 따라서 달라짐.

- Table of phoneme (U.S English), Common realizations

  ![1](https://user-images.githubusercontent.com/37765338/62823709-d1430c00-bbce-11e9-8bef-05bc8920e3b9.png)

- Unvoiced phonemes & Voiced phonemes로 구분됨.

  - Unvoiced phonemes : 성대와 연관이 없음 (do not engage the vocal cords) , 고유 진동수나 높이(pitch)를 가지지 않는다.

### 2-3. 모음(vowels)과 자음(consonants)

phoneme을 구분하는 가장 대표적인 방법

1. **Vowels** : 자음들은 아래 두가지 요소를 통해서 구분됩니다.

   - Voiced sounds 

     \- 성대(Vocal chords)에서부터 구강(Mouth cavity) 에서 나오는 **공기의 흐름**

     \- 성대의 진동을 통해서 생성됨.

     \- 고유한 진동수와 음높이(pitch)를 가지고 있음

   - 혀, 입술, 턱의 위치

     - Formants(모음의 구성 음소들) : 3가지 기관의 위치들로 하여금 생성되는 성도(vocal tract) 내에서의 소리의 공명(resonances) 

     - 이러한 Formant가 생성되는 공명의 진동수(resonant frequencies)에 따라서 모음을 구별할 수 있다.

2. Consonants : 기도(airway)나 입에서 나는 **공기의 흐름**에 따라서 구별된다.1

   - 몇몇 자음들은 **소리가 나느냐 안나느냐로만** 구분할 수 있다!

     ex) `/b/` & `/p` , `/d/` & `/t/`

     - 다른 조음 기관의 위치 (articulatory characteristics)들은 동일해서, 턱이나 입, 혀의 위치로는 이 두 발음을 구별할 수 없습니다.

### 2-4. 음성인식을 하기 위해서는..

**Context-dependent nature of phoneme**을 염두해야 한다!



## 3) Words and Syntax

### 3-1. Syllables and words

- **Syllable**(음절) : 시작음 (Initial phones) + 중심음 (nucleus phones) + 종결음 (final phones)
  - Nucleus phones는 보통 voiced sound이다 (vowel or syllabic consonant)

- Word는 Syllable의 상위 개념,

  ex) Word : `bottle` 

  - Syllable 1 : `b aa t` = voiced consonant (`b`) + nucleus (`aa`) + unvoiced consonanr (`t`)
  - Syllable 2 : `l` = Syllabic consonant

  물론 `Eye`같이 한 Syllable로 이루어진 단어도 있다!

- 음성인식 모델을 짜기 위해서는 보통 단어를 Syllable으로 나누기보다는 **phoneme들로 분리하는 경우**가 많습니다.

### 3-2. Syntax and Semantics

NLP에서는 두 요소가 중요하지만, 음성인식에는 **문법과 의미가** 그렇게 중요하지 않다!



## 4) Measuring Performance

음성인식이 잘 되었는지를 측정하는 기준

### 4-1. Word error rate

참고링크 : https://en.wikipedia.org/wiki/Word_error_rate

음성인식 답안(Reference)이 있고, 음성인식기를 통한 인식결과(Hypothesis)가 있을 때, 두 결과가 얼마나 다른지를 측정하는 방법 중 하나입니다.

- 오류의 종류 : **Insertion, Deletion, Substitution**

- Word error rate = (Insertion + Deletion + Substitution) / (Number of words in reference)

- Word error rate는 얼핏 보면 계산하기 쉬워 보이지만, 그렇지 않습니다.

  왜냐하면, **두 예측결과의 위치를 정확히 맞춰줄 필요가 있기 때문입니다.**

ex) 

Reference : however a little later we had a comfortable chat

Hypothesis : how never a little later he had comfortable chat

이런 식으로 되어있다면,

| Reference  | however |       | a    | little | later | we   | had  | a    | comfortable | chat |
| ---------- | ------- | ----- | ---- | ------ | ----- | ---- | ---- | ---- | ----------- | ---- |
| Hypothesis | how     | never | a    | little | later | he   | had  |      | comfortable | chat |

이런 식으로 배치하여 I, D, E 에러를 계산하는게 합리적인데,

| Reference  | however | a     | little | later  | we    | had  | a    | comfortable | chat |      |
| ---------- | ------- | ----- | ------ | ------ | ----- | ---- | ---- | ----------- | ---- | ---- |
| Hypothesis | how     | never | a      | little | later | he   | had  | comfortable | chat |      |

이렇게 순서대로 무작정 배치를 한다면, 맞게 예측한 단어도 Substiution으로 책정하기 때문입니다.



### 4-2. String edit distance

Word error rate를 계산하는데 쓰이는 방법은 보통 String edit distance입니다.

- [String edit distance](https://en.wikipedia.org/wiki/Edit_distance)

  : 두 문자열 A,B가 있을 때, **A에 최소 몇 번의 연산을 거쳐서 B와 동일하게 만들 수 있는지**를 측정하는 알고리즘입니다.

  - 이 때, 연산은 Deletion, Substitution, Insertion이 있습니다.
  - [Levenshtein distance](https://en.wikipedia.org/wiki/Levenshtein_distance)라고도 불립니다.

- 정확히 예측한 부분끼리는 Reference와 Hypothesis의 위치를 일치시키는 것이 합리적이기 때문에 **최소의 편집 횟수**를 구하는 것입니다.

- String edit distance를 구하는 방법 : 가장 쉬운 방법으로는 [Wagner-Fischer algorithm](https://en.wikipedia.org/wiki/Wagner–Fischer_algorithm) 가 있습니다. 



### 4-3. Wagner-Fischer algorithm

점화식을 이용하여 Edit distance를 구합니다.

1. 변수 정의

   - A(k) =  `a_1` ~ `a_k` 까지의 문자열 ( `0 <= k <= n`)
   - B(l)  =  `b_1` ~ `b_l` 까지의 문자열 ( `0 <= l <= m`)
   - `f(k, l)` = A(k)와 B(l)간의 Edit distance
   - Object = `f(n, m)`

2. 재귀식 정의

   - `f(0, l)` = l, `f(k, 0)` = k

   - `f(k, l)` 

     - Case 1 : `a_k` == `b_l` 

       - 이 때는, `f(k-1, l-1)`과 동일하게 계산할 수 있음.

     - Case 2 : `a_k != b_1`

       다음 3가지 경우 중에서 최솟값을 구하면 됩니다.

       1. `f(k-1, l)` + 1 : `a_k`를 제거하면 됨
       2. `f(k, l-1)` + 1 : `b_l`을 제거하면 됨
       3. `f(k-1, l-1)` + 1 : `a_k`을 `b_l`으로 바꾸면 됨

3. 알고리즘의 Correctness

   - `a_k`와 `b_l`을 처리하는 방법은 총 4가지입니다.
     1. `a_k`를 `b_l`로 대체하기
     2. 문자열 A에서 `a_k`를 제거하기 (B에서 `a_k`를 추가하는 것과 동일)
     3. 문자열 B에서 `b_l`을 제거하기 (A에서 `b_l`을 추가하는 것과 동일)
     4. 그대로 놔두기 : `a_k`와 `b_l`이 같은 경우에는 가능하다!

   - **만약 다른 방법을 시도한다면, 결국에는 조건을 만족하기 위하여 1~4중에 한 가지를 반드시 시행해야합니다.**

     ex) `a_k`와 `b_l-1`을 일치시킨다면, `b_l`을 반드시 제거해야합니다.

     (증명은 [여기](https://stackoverflow.com/questions/35835942/proof-of-correct-of-the-dynamic-programming-approach-to-min-edit-distance)에 있긴한데, 직관적으로만 이해하면 될 것 같습니다.)

   - 따라서, 최선의 경우의 수는 반드시 4가지 중에 하나이며, `2`와 같이 재귀적으로 최솟값을 구할 수 있습니다.

4. 알고리즘의 시간복잡도

   - 총 State의 수는 (m+1)(n+1)
     - 사실 `f(0, l)`과 `f(k, 0)`은 자명하기 때문에 나머지 mn개만 구하면 됩니다.
   - 한 State의 값을 구하기 위해서는 기껏해야 다른 3개의 State만 참고하면 됩니다. (Dynamic programming을 이용하면 가능하다!)

   => `최대 경우의 수 <= mn * 3`, 따라서 시간복잡도는 `O(mn)`

   

### 4-4. Measure의 종류

WER(Word error rate), SER(Sentence), CER(Character)

당연히 Character 기준의 error rate가 제일 높게 측정됩니다.



## 5) Other considerations

### 5-1. Statistical significance testing

두 실험(or 알고리즘)이 어느정도 차이가 있는지를 테스트하는 것

- 물론, 두 실험의 차이는 실제로 차이가 있어서 발생하는 것일 수도 있지만, 외부 요인으로 인한 차이점을 배제할 수 없습니다.
- 예를 들어서, **데이터의 다양성으로 부터 발생하거나**(우리의 데이터가 우연히 특별해서..) 두 실험을 정밀하게 하지 못하여 차이가 발생하는 것일 수도 있습니다.

이런 차이를 측정하는 방법은 당연히 task-dependent 입니다.

- 보통은, 통계적 가설검정을 주로 사용합니다.
- 두 음성인식 시스템의 차이를 비교하기 위해서는 보통 MAPSSWE를 사용합니다.([Matched Pairs Sentence-Segment Word Error](http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.296.4438))



### 5-2. RTF (Real time factor)

물론, 음성인식 시스템이 아무리 정확도가 높다고 해도 측정시간이 오래 걸리면 실제로 써먹기가 힘듭니다.

예를 들어서, RTF가 2라면(10초의 음성파일을 인식하는데 20초가 걸림), 실제 음성을 인식하는 속도보다 느리기 때문에 Real-time application(예, 스트리밍 서비스에서 사람의 음성을 인식하여 자막을 도출)에서는 사용하기 힘들 것입니다.

일반적으로 정확도와 Inference time은 trade-off이기 때문에 어플리케이션에 따라서 적절한 속도를 선택하고, 그에 맞는 알고리즘을 사용해야 할 것입니다.



## 6) Fundamental equation of ASR

기본적으로 음성인식은, **주어진 음성의 특징 벡터**에 대해서 **가장 가능성이 높은 문장**을 찾는 것이 목표입니다.

### 6-1.음성의 특징 벡터 

음성은 기본적으로 아날로그 데이터(연속함수)이기 때문에, 이 값을 전부 이용하기보다는, **음성을 요약하는 몇 가지 특징을 추출**하여 사용하게 되는데, 이를 특징 벡터라고 합니다.

### 6-2.Object function

`argmax(W)(P(W|O))`

- W = 단어들의 Sequence (문장)
- O = 특징벡터 (보통 Float 값의 Sequence)

이를 베이즈 공식(Bayes rule)을 이용하여 분해하면, **P(O|W) * P(W)**가 됩니다.

### 6-3. 음성인식의 3가지 Component

최적해를 구하기 위해서는 해당 식을 3가지 과정으로 분리하여 구합니다.

- Acoustic model : P(O|W)를 측정
  - 해당 문장을 표현하는 **음성의 distribution**을 도출하는 함수

- Language model : P(W)를 측정
  - 해당 **문장이 얼마나 자연스러운지**를 도출하는 함수 

- Decoder : 음향모델과 언어모델을 기반으로 Argmax를 계산
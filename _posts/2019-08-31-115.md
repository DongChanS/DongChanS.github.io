---
layout : post
title : Acoustic model (1) - HMM (Hidden markov model)
author : Shin Dong Chan
category : 'Speech Recognition'
---

Edx에서 제공하는 Automatic speech recognition 강의와, 

http://www.inf.ed.ac.uk/teaching/courses/asr/lectures-2019.html 의 강의자료를 참고하였습니다.

## 1) Introduction

이전에는 음파에서 음성인식을 위하여 중요한 특성을 추출하는 과정인 MFCC에 대해서 배웠습니다.

이렇게 추출한 MFCC는 음성인식에 있어서 중요한 정보가 되는데요, 이번에는 음성인식을 위한 과정 중 하나인 Acoustic modeling에 대해서 알아봅시다.

### 목차

1. Acoustic model(1편)
2. Acoustic model의 핵심 : HMM(1편)
3. MLE : Maximum likelihood estimation(2편)
4. GMM-HMM의 MLE 추정 & Inference (2편)
5. Context dependent phones(3편)
6. DNN-HMM (4편)

3편부터는 WFST(Weighted Finite State Transducer)에 대해서 알고 있어야 이해하기가 편하기 때문에 2편이 끝나고 WFST를 먼저 소개하도록 하겠습니다.   



## 2) Acoustic model

### 2-1. Acoustic model

Acoustic model은 주어진 문장에 대한 음성의 특징벡터(ex, MFCC)의 분포(distribution)를 계산해주는 모델입니다.

예를 들면, `'나는 음성인식을 한다.'` 라는 문장을 발음할 때,

사람마다 가진 고유의 **높낮이**(음정)은 다를 수가 있어도, 

같은 문장을 발음하는데에 있어서 생기는 **음파의 형태**(음색)와 **목소리의 크기 변화**(음량)는 비슷한 패턴을 가질 것입니다.

그래서 Acoustic model은, 많은 사람들의 발화데이터를 통해서 주어진 문장을 발음할 때에 나오는 **특징벡터의 패턴을 파악**하는 모델이라고 할 수 있습니다.

### 2-2. HMM 기반의 Acoustic model

![1](https://user-images.githubusercontent.com/37765338/64064287-67041100-cc3a-11e9-8de5-0f199338dd1c.png)

Generative model : 주어진 Input에 따라서 Output을 **확률적으로** 생성할 수 있는 모델.

- Input : Utterance
- Output : Acoustics (음파의 음향적 특징)

Generative model을 사용하는 이유는 뭘까요??

<br>

음성인식의 목적은, 특징벡터로부터 가장 적절한 Utterance를 예측하는 것인데,

가장 간단한 방법은 특징벡터에 대한 각 Utterance의 조건부확률을 반환하는 모델을 잘 학습하여 그대로 사용하는 것입니다.

하지만,  그 조건부확률을 직접 모델링하기는 쉽지가 않습니다.

흔히 Classification task에 사용되는 Fully connected layer + softmax를 잘 설계하려고 생각해봐도 생각보다 만만하지가 않습니다. 

그래서 우회하는 방식을 택하게 됩니다.

생성모델을 이용해서 Joint probability를 잘 설계할 수 있다면,  베이즈 룰에 근거하여 가장 적절한 Utterance를 예측할 수 있기 때문입니다.



## 3) Acoustic model의 핵심 : HMM

고전적으로 많이 사용되었던 모델은 GMM-HMM인데, 이는 Gaussian mixture model과 Hidden markov model의 줄임말입니다.

이 GMM의 단점을 극복하기 위해서 DNN을 이용한 DNN-HMM을 사용하기도 하는데, 어찌됬건 HMM은 아직도 사용되고 있습니다.

이번에는 왜 HMM이 Acoustic modeling에 그렇게도 사용되는지, 어떻게 사용되는지를 알아보도록 하겠습니다.

### 3-1. Hidden markov chain

먼저, Hidden markov model은 두개의 Stochastic process을 이용하여 어떤 현상을 모델링합니다.

* Stochastic process : Random variable들의 벡터

![4](https://user-images.githubusercontent.com/37765338/64064291-6b302e80-cc3a-11e9-81ca-9438af941cc2.png)

`X`는 **Markov process**입니다. 이는 직접적으로 관측이 되지 않기 때문에 **잠재변수**라고 부릅니다.

* [Markov process](https://en.wikipedia.org/wiki/Markov_chain) : Markov property를 따르는 Stochastic variable

  (우선 위의 링크를 읽으시고 Transition probability, State라는 키워드에 대해서 학습하실 것을 추천드립니다. 여기서는 이에 대해서 상세히 설명할 수 없습니다 ㅠㅠ)

  * Markov property : 현재의 Random variable은 **바로 이전**의 Random variable에만 의존하는 성질
  * 보통, Time-Homonogenuous markov chain을 사용하는데, 이는 Time에 따라서 Transition probability가 변하지 않는다고 가정하기 때문에, **Transition probability를 행렬으로 표현**할 수 있습니다.

* 직접적으로 관측이 되지 않는다 = HMM으로부터 직접 관측할 수 없다

  = HMM의 결과값으로 생성되지 않는다.

`Y`는 HMM이 생성하는 확률변수입니다. 다만, `Y`를 생성하는 데에는 **오직 `X`만 관여한다는 특징**이 있습니다.

* 그것도 모든 State가 아니라, 동일한 State의 X 확률변수만 관여한다는 점입니다.

  ![1-5](https://user-images.githubusercontent.com/37765338/70845409-a6750700-1e91-11ea-8f91-4a5e26ed8b4a.png)

  이는 매우 강력한 조건입니다. 일반적인 현상 (이를테면 직장인의 월급과 월 지출간의 관계)는 이런 조건을 만족하지 못하기 때문에, HMM을 통해서 모델링하게 되면 매우 위험할 수 있습니다. 

* 왜냐하면.. 월 지출은 자신의 월급 이외에도 다양한 외부 요인(ex, 부채)이 많을 뿐더러, 몇 년을 모아서 한번에 구매하는 경우도 있기 때문에, 바로 이전 달의 월급만으로는 잘 판단하기 힘들 것입니다.



### 3-2. 여러개의 HMM을 학습

보통 머신러닝 모델을 만든다고 하면, 하나의 큰 모델을 설계하고 다량의 데이터를 통해서 학습을 하는 경우가 많은데,

고전적인 음성인식에서는 **여러개의 HMM**을 학습하여 사용합니다.

왜냐하면 하나의 HMM으로는 모든 utterance에 대한 성질을 반영하기 힘들기 때문입니다.

이를테면 아래와 같이 단어별로 HMM을 학습해서 사용할 수 있겠습니다.

![1](https://user-images.githubusercontent.com/37765338/70845417-c1e01200-1e91-11ea-8e36-833fab7219ea.png)

그러면 해당 특징벡터가 들어오면 전부 HMM에 투입해보고, 가장 확률이 높은 HMM에 대한 단어를 예측값으로 생각하면 됩니다.

그런데 이러한 접근법은 몇 가지 치명적인 단점이 있습니다.

1. Scalable하지 않다.
   * 단어의 종류는 엄청 많기 때문입니다... HMM이 엄청 많이 필요합니다
2. 각 단어마다의 사용 비율이 다르다.
   * 어찌됬건 HMM을 학습을 해야하는데, 사용비율이 낮은 단어에 대한 HMM은 잘 학습이 되지 않을 수 있습니다.
3. 새로운 단어가 생길 수도 있다.
   * 가장 큰 문제입니다. 단어 사전을 기반으로 HMM을 만들텐데, 사전에 없는 단어로 발음한다면 그 단어로는 절대로 인식할 수 없습니다.

그래서 **기본 단위**를 사용해야합니다.

바로 음성의 기본 단위인 **Phoneme**입니다.

영어에서 Phoneme는 종류가 44가지이기 때문에 위의 문제를 만족할 수 있습니다.

* 왜 한글 기준이 아니냐면.. 대부분 개발된 HMM Toolkit이 영어 기준이기 때문입니다. 물론 한글 발음을 영어 Phoneme로 매칭할 수 있기 때문에 큰 문제는 되지 않습니다.

그렇게 각 Phoneme마다 HMM을 만들어서, 시간마다 Phoneme을 잘 예측한다면 나중에 단어로 합치는 과정도 필요하겠지요. 

그것을 Pronunciation model (발음사전)이라고 합니다. 

![1-2](https://user-images.githubusercontent.com/37765338/70845418-c4426c00-1e91-11ea-9dda-c5c1e7df5c32.png)



### 3-3. Context-dependent phone

하지만, 각 Phoneme마다 HMM을 구분하면 생기는 문제점이 있습니다.

왜냐하면 주변 발음에 의해서 Phoneme의 발음이 달라지기 때문입니다.

( ex) `입학`과 `각하`의 `ㅎ` 발음을 생각해보세요! ) 

만약, `입학`의 `ㅎ`랑 `각하`의 `ㅎ`를 같은 Phoneme으로 묶어서 HMM을 학습하게 된다면 HMM이 서로 다른 발음을 학습하는 것에 치우쳐서 다양한 사람의 발음에 대한 패턴을 잘 학습하지 못하게 될 것입니다.

그래서 Phoneme가 아니라 **Phone마다 HMM**을 구분해야합니다.

* **Phone** : acoustic realization of phoneme, 즉 주변의 발음을 반영한 Phoneme입니다.
* Context-dependent phone이라고도 부릅니다.

![6](https://user-images.githubusercontent.com/37765338/64064293-6bc8c500-cc3a-11e9-8f37-7b174f70daa5.png)

주변의 발음을 반영하는 방법은 간단합니다.

바로 이전, 이후에 발음했던 Phoneme에 따라서 구분하는 것이죠.

예를 들면, `ax -> b -> ah` 이런 순서대로 발음했다면 오직 이 발음에 대해서만 하나의 모델을 학습하는 것이죠. 

![7](https://user-images.githubusercontent.com/37765338/64064294-6bc8c500-cc3a-11e9-89eb-22658e9da8f3.png)

일반적으로는 Phone을 `ax-b+ah`와 같이 표시합니다.

문제는, Phoneme의 종류가 40가지이기 때문에 phone의 개수는 `40*40*40 `= 64000이 되는데, 64000개의 HMM을 학습하기에는 너무 비효율적입니다.

(잘 등장하지 않는 발음에 대해서는 학습이 잘 되지도 않습니다.)

그래서 phonetic decision tree를 이용해서 **비슷한 성질의 phone끼리는 HMM을 공유**하는 작업을 할 수도 있습니다.



### 3-4. HMM에서 State의 의미

![3](https://user-images.githubusercontent.com/37765338/64064289-68353e00-cc3a-11e9-8dff-9ec64743b872.png)

음성인식에서 쓰이는 HMM은 left-to-right model입니다.

left-to-right markov chain은 시간에 따라서 State를 역행하지 않는 model을 말합니다.

그렇게 되면, 아래와 같이 각 state는 서로 독립적인 시간대의 Observation vector를 반영하는 셈이죠.

![2-3](C:\Users\PC\OneDrive\바탕 화면\2-3.png)

예를 들어 한 프레임당 20ms의 시간을 가진다고 한다면,

0 - 60 ms : State 1

60 - 100 ms : State 2

100 - 120 ms : State 3

이런 식으로 매칭되어 State의 순서와 Phone의 발음**순서가 일치**하게 되기 때문에, 

S1 -> S2 -> S3으로 진행할수록 Phone의 발음 변화를 순차적으로 반영하게 되는 것입니다.

즉 S1은 left context를 반영하고, S2는 middle context, S3은 right context를 반영하게 될 것입니다.

이러한 구조를 **연음 구조(Prelonged sound)**라고 합니다.



결국 State의 개수가 많아질수록 이 발음변화를 디테일하게 반영할 수 있지만, 일반적으로는 3~5개가 적절하다고 판단합니다.

이유는 State의 개수가 많아질수록 학습해야 할 파라미터가 많아져서 비효율적이기도 하고, 

애초에 특징벡터를 추출할 때, 짧은 시간대에서는 Stationary하다는 가정을 하기 때문에 특징벡터에 원 음성의 연음구조가 잘 반영되지 않기 때문이기도 합니다. 



### 3-5. HMM의 적절성

앞서서, Hidden markov model에는 강력한 조건이 붙는다고 하였습니다.

두 가지 조건을 모두 만족하는 좋은 모델이라고 할 수 있습니다.

* Markov property : State간 전이확률은 오직 바로 이전 State에만 의존한다.

  => 발음은 순차적으로 이루어지기 때문에 바로 직전의 발음만 고려해도 충분합니다. (O)

* Observation independency : 각 시간의 Observation vector의 생성확률은 그 시간에 해당하는 State에만 의존한다.

  => 해당하는 프레임의 특성벡터는 그 때의 발음, 연음구조에 따라서 생성된다 (O)








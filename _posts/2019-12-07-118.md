---
layout : post
title : 내맘대로 논문 요약하기 - ELMo
author : Shin Dong Chan
category : 'NLP'
---


ELMo는 Embeddings from language models의 약자인데요, 말그대로 language model을 학습함으로써 각 token에 대한 embedding을 만드는 방식입니다.

이 ELMo도 Bert 논문에서 대조군으로 등장한 모델입니다.

Bert를 알아보기에 앞서서, 기존에는 어떤 식으로 Pre-training language repesentation을 만들었는지를 파악해봅시다.

![img](https://wikidocs.net/images/page/33930/elmo_DSHQjZD.png)

[ELMo Github](https://github.com/allenai/allennlp/blob/master/tutorials/how_to/elmo.md), [논문 링크](https://arxiv.org/abs/1802.05365)

## 1) Abstract

* New type of contextualized word representation

* ELMO는 단어들에 대한 복잡한 특성을 잘 모델링했습니다.
  * 대표적인 복잡한 특성으로는 문법구조를 적절히 파악하는 것, 다의어의 경우 어떠한 뜻으로 이용되었는지를 파악하는 것이 있겠습니다.
* Deep bi-directional language model을 통해서 워드벡터들이 사전학습됨
  * 이러한 사전학습된 벡터를 각 Task를 수행하는 **모델에 첨가**하여 정확도를 높임



## 2) Introduction

워드벡터의 사전학습은 매우 중요하지만 기존의 Skip-gram과 같은 방법들로는 문법구조, 다의어에 따른 뜻 변형을 적절히 반영하기에 어려운점이 있음

ELMo는 사전학습된 Contextualized word vector를 만들고, 이를 기존 모델에 잘 통합할 수 있도록 하는 것이 목적



ELMo는 Language model objective 를 통해서 학습된 Bi-lstm을 통해서 워드벡터가 만들어진다는게 다른 방법론과의 차별점.  

* 심지어, lstm의 마지막 레이어만 사용하는 기존의 방법론과는 달리, **ELMo는 lstm의 모든 내부 레이어를 사용**해서 만들어지기 때문에 더더욱 많은 정보를 사용할 수 있다.



저자는 Intrinsic evaluation을 통해서 다음과 같은 특징을 파악할 수 있었음.

(Intrinsic evaluation은 만들어진 임베딩 벡터를 평가할 때, Named entity recognition과 같은 Real task의 정확도를 통해서 임베딩의 품질을 평가하는 것이 아니라, Word analogy와 같은 Subtask를 구축하여 임베딩의 품질을 평가하는 방식입니다. )

* higher-level LSTM : 문맥을 반영한 단어의 의미를 잘 표현함 
* lower-level LSTM : 단어의 문법적인 측면을 잘 표현함



## 3) Related work

1. 전통적인 Word embeddings (ex, Skip-gram)

   * Context-independent representation

2. 전통적인 워드임베딩의 단점을 극복하기 위한 방법들

   1. Subword information (ex, Fasttext )

      * ELMO에서는 character convolution을 사용하기 때문에 일종의 subword information을 반영한다고 할 수 있음.

   2. learning separate vectors for each word sense 

      * word sense라는 별도의 벡터를 통해서, 이 단어가 어떤 카테고리에 속하는지를 판단하는듯합니다.

      * ELMO에서는 직접적으로 multi-sense class를 명시하여 그에 대한 벡터를 학습하지는 않지만, 문맥을 파악하기 때문에 multi-sense information을 잘 반영한다고 할 수 있습니다.

3. Context-dependent representations

   Unsupervised language model의 인코더를 사용해서 중심 단어의 contextual embedding을 만드는 방식입니다.

   * Context2vec : Bi-LSTM을 이용해서 중심 단어 주변의 context를 인코딩함
   * CoVe : 기계번역 시스템의 인코더를 사용해서 중심 단어의 contextual embedding을 만듬.

   => 위의 두 방식은 학습 데이터가 많이 필요한데, 특히 CoVe는 기계번역에 필요한 parallel corpora을 수집하기 힘들기 때문에 다소 비현실적이다.

4. Encoder-decoder 구조를 사전학습하는 방식

   * Language model objective이나 Sequence autoencoder를 통해서 language model을 사전학습하고, 각 Task에 맞춰서 파인튜닝하는 방식이다.

     (이전 포스팅인 OpenAI-GPT가 이 케이스입니다.)

   * 이러한 방식과는 달리, ELMo에서는 Universal representation을 가지고 있기 때문에 파인튜닝하는데 쓰일 트레이닝 데이터가 적다면, 이 universal representation의 비중을 늘리도록 할 수 있다는 장점이 있다.



## 4) ELMO의 구조

### 4-1. Bidirectional language models

기존의 접근방식을 요약하자면..

1. 캐릭터 컨볼루션 네트워크를 통해서 캐릭터 단위의 임베딩을 만든다.

2. 두개의 목적함수의 합으로 최종 목적함수를 만들고, 이를 기반으로 각 모델의 파라미터를 학습함.

   * Top layer of Left-to-right lstm  + forward LM
   * Top layer of Right-to-left lstm + backward LM

   ![ELMO1](https://user-images.githubusercontent.com/37765338/70372520-71ae0080-1923-11ea-95c5-ea446cb5cb15.png)

3. 특이한 점이 있다면, 두 개의 LSTM 모두 같은 Token embedding과 같은 Softmax layer를 공유한다는 점입니다.
   
   
   

### 4-2. ELMo

ELMO는 다음과 같은 방식으로 만들어집니다.

1. 위와 동일한 방식으로 두개의 LSTM 모델을 학습한다.

2. 각 Token의 Contextual embedding은 양방향 LSTM의 모든 레이어와 토큰임베딩을 **전부 사용하여** 만들어집니다.

   ![ELMO2](https://user-images.githubusercontent.com/37765338/70372521-71ae0080-1923-11ea-821b-b72c10810d7d.png)

3. 이를 적절히 일차결합하여 **downstream application의 feature로** 사용한다.

   ![ELMO3](https://user-images.githubusercontent.com/37765338/70372519-71ae0080-1923-11ea-86db-908af439ff6a.png)

   * `r_task` : 전체 elmo vector를 스케일링하기 위한 파라미터 (각 Task마다 어느정도의 가중치로 ELMO벡터를 반영할지를 판단하는듯)

   * `s_task` : 각 레이어를 반영하는 weight

     * 각 레이어의 분포(distribution)가 다르기 때문에 Layer normalization을 수행하면 더 정확하게 weight를 반영할 수 있습니다.

     * Task에 따라서 `s_task`의 값들은 달라질 수 있습니다.

       문법적인 정보가 많이 필요한 task라면 아랫단에 가중치를 크게 두고, 의미적인 정보가 많이 필요한 task라면 윗단에 가중치를 크게 두는 것이 좋을것입니다.

     

### 4-3. Using biLMs for supervised NLP tasks

ELMO embedding을 첨가하는 방식은 각 Task를 수행하는 모델에 따라서 달라질 수 밖에 없는데, 

만약 lowest layer에 Task RNN을 사용한다면,  각 토큰의 embedding들에 ELMO 벡터를 concatenate하여 인풋으로 사용할 수 있을 것입니다.

그렇게 하면, 기존 supervised 모델의 weight를 건드리지 않고 추가적인 feature를 넣을 수 있습니다.



## 소감

ELMo는 OpenAI GPT와는 달리 Bert와 다른 점이 많습니다.

하지만, 비교적 오래 전에 등장한 Bi-LSTM을 통해서 우수한 Pre-trained language model을 만들었다는 점에서 ELMo를 눈여겨 볼 만한 듯 합니다.
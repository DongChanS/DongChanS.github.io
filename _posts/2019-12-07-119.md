---
layout : post
title : BERT에 대해서 자세히 알아보기 (1) - 선행학습 
author : Shin Dong Chan
category : 'NLP'
---


![1](https://user-images.githubusercontent.com/37765338/70374461-d1ada280-1935-11ea-8d96-65507508c793.png)

BERT는 앞서 소개한 OpenAI GPT와 ELMo와 같은 Pre-trained language model 입니다.

이 BERT를 이용한 모델들이 [KorQuad]([https://korquad.github.io/KorQuad%201.0/](https://korquad.github.io/KorQuad 1.0/)) 와 같은 각종 Competition에서 우수한 결과를 뽐내고 있는 만큼, 이는 매우 중요한 모델이라고 할 수 있습니다.

[BERT Github](https://github.com/google-research/bert), [BERT 논문](https://arxiv.org/abs/1810.04805)

이번에는 BERT의 개요와 함께, 이해해야할 개념들에 대해서 먼저 알아보도록 하겠습니다.



## Abstract

![2](https://user-images.githubusercontent.com/37765338/70374463-d2decf80-1935-11ea-9eca-9032bf3b06e0.png)

BERT 논문의 Abstract를 보면 다음과 같은 말들이 있습니다.

* BERT는 New **language representation model**이다.
* BERT는 **Pre-train** deep **bidirectional** representations로 설계되었다.
* Pre-trained BERT에 하나의 레이어만 잘 추가해주면 State-of-the-art models for a **wide range of tasks**를 낼 수 있다.

논문을 더 보기에 앞서서, 위에서 말한 내용들이 무엇인지를 이해해보도록 합시다.



## 1 - Language representation model

Language representation model이란, 말 그대로 **언어를 표현**할 수 있는 머신러닝 모델이라는 뜻입니다.

기계는 숫자밖에 모르기 때문에, 언어를 표현한다는 뜻은 `말 -> 벡터`로 바꿔주는 역할을 잘 한다는 뜻이 되겠습니다.

이렇게 언어를 잘 표현하는 머신러닝 모델을 만드는 방법에는 여러 가지가 있겠지만, 대표적으로는 **Language model**이 있습니다.



### 1-1. Language model

![3](https://user-images.githubusercontent.com/37765338/70374465-d5412980-1935-11ea-8eb2-259ceeae092e.png)

[Language model](https://en.wikipedia.org/wiki/Language_model)은 함수입니다.

어떤 문장을 받으면, 이 **문장에 대한 확률값**을 반환하는 벡터인데요,

확률이 높다는 뜻은, 여러 가지로 해석할 수 있겠지만 이 문장이 개연성이 있다는 뜻을 의미합니다.

문장에 대한 확률을 계산해야 하기에, 여러 토큰(단어)들에 대한 **결합확률**을 계산해야 하는데요, 이러한 결합확률을 계산하는 방법에 따라 여러 방법으로 분류할 수 있습니다.



1. `N-gram based model`

   ![4](https://user-images.githubusercontent.com/37765338/70374466-d5412980-1935-11ea-8eaa-74140a97d536.png)

   N-gram based model은 문장의 각 토큰들에 대한 확률을 결합할 때,

   **해당 토큰에 대한 확률은 이전에 등장한 N개의 토큰들에만 의존한다.**는 가정을 하여 계산됩니다.

   (물론 Skip-gram과 같은 Windowing 접근법을 이용해서 양쪽의 N개 토큰에 대한 결합확률을 계산하는 방법도 N-gram based model이라고 할 수 있습니다.)

   이 방법이 최고는 아니겠지만, 여러개의 단어들을 고려하는 데서 나오는 계산량을 줄이기 위해서 나온 차선의 방법이라고 할 수 있습니다.

   대표적인 예로는 Markov model, OpenAI GPT 등이 있겠습니다.

2. `Autoregressive language model` (Unidirectional)

   ![5](https://user-images.githubusercontent.com/37765338/70374467-d5d9c000-1935-11ea-93f5-3694df24c762.png)

   Autoregressive language model은,

   **해당 토큰에 대한 확률은 이전에 등장한 모든 토큰에 의존한다.**는 가정을 합니다.

   BERT 이전에 나온 많은 머신러닝 모델들이 이 방법을 채용했는데요,

   대표적으로는 Seq2Seq, ELMo 등이 있겠습니다.

3. `Bidirectional language model`

   ![6](https://user-images.githubusercontent.com/37765338/70374468-d5d9c000-1935-11ea-8873-dde062cdb7ce.png)

   해당 토큰에 대한 확률을 계산할 때, 앞 뒤 모든 토큰을 반영하여 계산하는 모델입니다.

   대표적인 예로는 BERT가 있습니다.



### 1-2. Unidirectional vs Bidirectional

그래서 위의 Language model이 어떤 차이를 가지고 있는지 체감이 잘 안되실 것 같아서 예시를 준비했습니다.

자연어 처리 분야, 머신러닝 모델을 학습한다는 것은 **빈칸추론** 문제를 지속적으로 푼다는 것을 의미합니다. 

이를테면 이런 문장에서 말입니다.

![7](https://user-images.githubusercontent.com/37765338/70374469-d5d9c000-1935-11ea-9009-fc6fce286147.png)

여기서 are에 해당하는 부분을 가려놓고, 어떤 단어가 올 것인지를 예측하는 문제를 푸는 것인데,

우리가 빈칸추론 문제를 풀 때는, 당연히 빈칸 이전의 앞 뒤 문맥(Right context, Left context)들을 근거로 하여 추론할 것입니다.

이러한 방식으로 빈칸추론 문제를 푸는 방법을 Bidirectional language model이라고 합니다.

![8](https://user-images.githubusercontent.com/37765338/70374479-fb66c980-1935-11ea-9fc0-402b684881ca.png)

반면에, Autoregressive language model과 같은 모델들은 다릅니다.

오직 빈칸 앞부분의 단어들만 이용해서 빈칸을 추론하는 식인데요, 

이렇게 빈칸추론 문제들을 학습하면 다소 이상할 수 밖에 없을 것입니다.



### 1-3. 결론

Language model은 문장이 주어지면 이 문장이 얼마나 자연스러운지를 판단하는 함수입니다.

좋은 함수를 만들기 위해서 기계로 하여금 빈칸추론 문제를 지속적으로 풀게 하는데, 빈칸추론 문제를 푸는 방식에 따라서 함수의 질이 달라질 수 밖에 없습니다.

이렇게 문장의 개연성을 잘 판단할 수 있다면, 기계는 문맥을 이해한다고 볼 수 있는데요,

이 **문맥에 해당하는 부분**을 잘 떼어낸다면 여러 자연어 처리 분야의 Task에 적용할 수 있겠지요.

이러한 부분을 Language representation model이라고 합니다.



## 2 - Pre trained model

당신은 한국어를 아예 모르는 사람에게 한국의 베스킨라빈스에서 아르바이트를 시키려고 합니다.

물론 아르바이트를 하면서 손님들의 반응을 감으로 캐치하여 훌륭한 일꾼이 될 수 있겠으나, 출근 이전에 **한국어를 배운 다음**에 **아르바이트를 하면서 일을 익히는 것이** 최선일겁니다.

여기서 **먼저 한국어를 배우는** 행위가 일종의 Pre-training, **아르바이트를 하면서 일을 익히는** 행위가 Fine-tuning이라고 할 수 있습니다.



다시 머신러닝으로 들어가서 이야기하겠습니다.



### 2-1. 왜 Pre-training이 머신러닝에 필요한가?

당신은 한국어로 문장을 적으면 영어로 번역해주는 번역기를 만드려고 합니다.

한영 번역기 모델을 학습하려면 학습 데이터가 필요한데,

이 학습 데이터는 `(한국어 문장, 번역된 영어 문장)`의 쌍으로 존재할 것입니다.

문제는 이 학습 데이터를 만드려면 일일이 영어 번역가가 번역을 해놓아야 한다는 점이죠.

이러한 parallel corpora는 굉장히 수가 적기 때문에 데이터를 구하기가 힘듭니다.



반면에, 일반적인 텍스트는 쉽게 구할 수 있습니다. 위키피디아 덤프파일과 같이 텍스트파일들을 뭉쳐놓은 공개된 corpora가 많기 때문입니다.

만약 이런 Unlabeled text를 통해서 머신러닝 모델을 학습을 할 수 있다면 엄청 효율적이겠지요.

하지만, label이 없이는 클러스터링과 같은 작업밖에 할 수 없기 때문에 어찌됬건 지도학습 모델을 이용해야합니다.



### 2-2. Unlabaled text로부터 학습하는 방법

바로 Unlabeled text를 통해서 Labeled text를 **자동으로 생성**하여 지도학습에 사용하는 것입니다.

대표적인 방법으로는 빈칸을 뚫는 방법이 있습니다.

문장에 랜덤한 위치에 빈칸을 만들고, `(빈칸이 있는 문장, 답안)`의 형태로 학습데이터를 생성할 수 있기 때문입니다.

이렇게 만든 학습데이터들을 통해서 **빈칸추론 문제**를 지속적으로 풀게 하면 좋은 Language model을 만들 수 있습니다.

위의 방법은 Unlabeled data를 가지고 지도학습을 하기 때문에 **Semi-supervised learning**이라고 부릅니다.

이러한 Semi-supervised learning의 대표적인 예로는 Word embedding을 학습하는 방식 중 하나인 Skip-gram이 있습니다.



### 2-3. 결론

Parallel corpora와 같은 Labeled data는 구하기가 어렵다.

대신에 일반적인 텍스트(Unlabeled data)는 구하기 쉽기 때문에 대신에 활용할 수 있다.

활용하는 방법은 빈칸추론용 데이터를 생성하여 빈칸추론 문제를 풀게 하는 것이며,

이러한 방법으로 좋은 Language representation model을 획득할 수 있다.

이렇게 만들어진 Language representation model을 활용한다면, 기계번역과 같은 NLP Task의 학습을 효율적으로 할 수 있다.

![1_C0X5BA3GYUwK9pkPGwvJbg](https://user-images.githubusercontent.com/37765338/70374500-4b459080-1936-11ea-8f45-674933943fd8.png)

출처 : https://medium.com/merantix/applying-deep-learning-to-real-world-problems-ba2d86ac5837
---
layout : post
title : 내맘대로 논문 요약하기 - OpenAI GPT
author : Shin Dong Chan
category : 'NLP'
---

OpenAI GPT는 최근 1년간 핫한 [Bert](https://arxiv.org/abs/1810.04805)를 소개하는 논문에서 비교군으로 등장한 모델 중에 하나입니다.

Bert를 알아보기에 앞서서, 기존에는 어떤 식으로 Pre-training language repesentation을 만들었는지를 파악해봅시다.

[논문 링크](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf)

## 1) Abstract

* Pre-training language model의 필요성 :

  라벨링이 되어있는 데이터는 라벨링이 되어있지 않은 양질의 데이터에 비해서 매우 적다.

  그래서 비지도학습을 통해서 Language model을 학습할 수 있다면, 지도학습에 비해서 매우 효율적일 것입니다.

* OpenAI GPT에서는 라벨링이 되어있지 않은 데이터로부터 비지도학습을 먼저 한 후에(generative pre-training), 각 Task에 대한 지도학습(discriminant fine-tuning)을 통해서 Task를 수행할 수 있는 능력을 기르는 방식을 택했습니다.

* 특히, OpenAI GPT는 fine-tuning 도중에 task-aware input transformation를 사용하기 때문에 모델의 구조 자체에 큰 변화를 주지 않고도 파인튜닝을 할 수 있습니다.



## 2) Introduction

대충 라벨링이 된 데이터를 얻는 것은 매우 힘들다는 내용..

그리고, Word-level representation에서는, 사전학습을 먼저 수행하면, 각 Task에 대한 성능이 매우 올라간다는 것을 경험적으로 보았음.

하지만, Word 단위 이상으로 사전학습을 하는 것은 상당히 힘든데, 대표적으로 두 가지 문제점이 있음.

1. 문장 단위를 학습하기 위한 적절한 목적함수에 대한 논의가 확실시되지 않음
2. 각 Task마다의 최적의 파인튜닝 방법(Transfer representations)에 대한 합의가 확실시되지 않음



 두번째 문제점에서, 여러 파인튜닝 방법들의 예시를 들면,

* 모델의 아키텍쳐 자체를 많이 변화시키는 방식
* 파인튜닝을 할 수 있는 보조격의 목적함수를 추가

.. 등등이 있는데, 이런 여러 방법중에서 어떤 방법이 제일 좋은지를 모르기 때문에 이는 semi-supervised learning의 성능을 올리는데에 대한 장애물이 됨.



이번에 소개할 OpenAI GPT의 특징은 다음과 같음

* semi-supervised learning ( pre-training + fine-tuning )

  1. Pre-training

     : language modeling 목적함수를 이용해서 다량의 라벨링되지 않은 데이터로 하여금 적절한 파라미터의 초기값을 찾도록 할 것임.

  2. Fine-tuning : 각 Task에 맞는 지도학습 목적함수를 통해서 파라미터를 적응학습할 것임.

* 사전학습을 통해서 universal representation을 만듦으로써, 여러 Task에 잘 적용할수 있도록 할 것임.

* 파인튜닝이 사전학습된 모델의 구조를 크게 변화시키지 않음.



이를 위해서 사용한 방법은 몇 가지가 있는데,

1. Transformer

   : long term dependencies를 보존할 수 있는 기억력이 좋은 네트워크이기 때문에 여러 Task에 대한 좋은 transfer performance를 낼 수 있음.

2. Task-specific input adaptations ( traversal-style )

   : 텍스트를 따로 구조화시키지 않고 그저 토큰들의 sequence로 보는 방식



## 3) Related Work

### Semi-supervised learning

* OpenAI-GPT는 이 유형에 속함
* 기존의 연구들도 이번과 마찬가지로, 사전학습을 통해서 적절한 feature를 구성하고, 그 feature를 이용해서 지도학습을 잘 하는 방식이지만, 우리는 Word-level information 이상의 정보를 사전학습을 통해서 얻어낼 것이다.
* 최근에는 우리랑 마찬가지로  구절 단위나, 문장 단위의 임베딩을 사전학습을 통해서 구현하는 방법들이 진행되었음.



### Unsupervised pre-training

* Semi-supervised learning의 특별한 케이스로, 주로 좋은 초기 파라미터를 찾는데 주력함.

* 예전에는 regularization 정도의 역할 (더 좋은 일반화 성능을 내는 역할)만 하였지만, 요즘에는 뉴럴네트워크의 학습 과정에 도움이 되는 방법론들이 개발되었음.

* 그나마 OpenAI-GPT와 비슷한 방식으로는, language model objective를 이용해서 사전학습하고, 그것을 지도학습을 통해서 파인튜닝을 하는 것들이 있는데 그나마 LSTM을 이용한 케이스라서 긴 문장에 취약하다는 단점이 있다.

* 다른 접근방식으로는 사전학습을 통해서 지도학습을 위한 보조적인 feature를 학습하는 것들이 있는데, 이는 각 Task마다 모델 구조의 많은 변화를 필요로 하기에 우리의 방법이 좀 더 좋음.

  

### Auxiliary training objectives

기존의 목적함수에 각 Task를 학습하기에 적절한 목적함수를 추가하는 방식들이다.



## 4) Framework

### Unsupervised pre-training

* N-gram 목적함수를 이용함

  ![Screenshot from 2019-12-03 11-37-58](https://user-images.githubusercontent.com/37765338/70372248-d1a2a800-191f-11ea-81e7-79cb4349b73f.png)

* Stochastic gradient descent를 통해서 최적화

* Multi-layer Transformer decoder 를 사용함.

  ![Screenshot from 2019-12-03 11-13-52](https://user-images.githubusercontent.com/37765338/70372244-d10a1180-191f-11ea-8667-e68ba296452a.png)

  * 이전 K개의 토큰을 통해서 현재의 토큰이 무엇인지를 예측하는 과정을 반복함.

  * Transformer의 디코더 부분을 사용하지만, 기존의 인코더와 디코더끼리의 attention 부분이 제거되었음.

    ![Screenshot from 2019-12-03 11-23-59](https://user-images.githubusercontent.com/37765338/70372246-d10a1180-191f-11ea-8b50-51ec8d57c89d.png)![Screenshot from 2019-12-03 11-23-44](https://user-images.githubusercontent.com/37765338/70372245-d10a1180-191f-11ea-84a1-a2b9cc7d91bb.png)

    (왼쪽: OpenAI GPT에 사용된 Transformer의 디코더 부분, 오른쪽: 기존 Attention is all you need 논문에 소개된 Transformer의 디코더 부분 )

  



### Supervised fine-tuning

* 목적함수

  1. Classification 목적함수 :![Screenshot from 2019-12-03 11-29-25](https://user-images.githubusercontent.com/37765338/70372247-d10a1180-191f-11ea-8acc-6d70c4ff5c46.png)
  2. Language modeling 목적함수 : 위에서 사용했던 n-gram 기반의 목적함수![Screenshot from 2019-12-03 11-37-58](https://user-images.githubusercontent.com/37765338/70372248-d1a2a800-191f-11ea-81e7-79cb4349b73f.png)

  * 두개의 목적함수를 병합하여 사용함, 병합하는 비율은 hyperparameter로 두었습니다.
  * Language modeling 목적함수를 Supervised fine-tuning에도 사용하는 이유는, 학습속도를 빠르게 하기 위함이라고 합니다.

* Fine-tuning을 할 때에는 각 Task마다 어떤 식으로 학습모델을 구성할지가 매우 중요한데, OpenAI GPT에서는 위에서 소개한 traversal style approach을 사용했습니다.

  ![Screenshot from 2019-12-03 11-40-20](https://user-images.githubusercontent.com/37765338/70372249-d1a2a800-191f-11ea-9f3a-8cff1ec0553b.png)

  즉, 두 개의 문장을 통해서 학습해야한다면 Tokenize된 각 문장을 일렬로 이어서 input data로 사용하는 셈입니다.

  

  

  
  

## 소감

OpenAI GPT와 Bert랑은 비슷한 점이 많은 것 같습니다.

특히, traversal-style approach나, Transformer를 통해서 language representation model을 학습시켰다는 점입니다.

다른 점은 대표적으로 masked language model이 있겠는데요, 이는 다음에 Bert를 소개할 때 알아보도록 하겠습니다.

